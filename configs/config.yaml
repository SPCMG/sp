data:
  data_root: "/home/jiaxu/projects/sp-mdm/dataset/HumanML3D"                        # Root directory for data
  motion_dir: "/home/jiaxu/projects/sp-mdm/dataset/HumanML3D/new_joint_vecs"        # Directory containing motion .npy files
  text_dir: "/home/jiaxu/projects/sp-mdm/dataset/HumanML3D/texts"                   # Directory containing text .txt files
  event_text_dir: "/home/jiaxu/projects/sp-mdm/dataset/HumanML3D/event_texts"       # Directory containing event text .json files
  min_motion_length: 40               # Minimum number of frames in motion data, i.e., 2s
  max_motion_length: 200              # Maximum number of frames in motion data, i.e., 10s
  max_shuffled_texts: 5               # Maximum number of shuffled texts to use for training
  max_text_length: 60                 # Maximum number of words in text data
  n_joints: 22                        # Number of joints in the motion data
  n_feats: 263                        # Number of features per joint (e.g., x, y, z, velocity_x, velocity_y, velocity_z)

train:
  batch_size: 32                      # Batch size for training
  shuffle: true                       # Shuffle the training data
  num_workers: 4                      # Number of subprocesses for data loading
  learning_rate: 1e-4                 # Learning rate for optimizer, 1e-4
  num_epochs: 300                     # Number of training epochs, 300

model:
  motion_encoder: "transformer"       # Type of motion encoder to use
  text_encoder: "clip"                # Type of text encoder to use
  latent_dim: 512                     # Dimension of the latent space
  num_layers: 4                       # Number of transformer layers in motion encoder
  num_heads: 4                        # Number of attention heads in motion encoder
  ff_size: 1024                       # Feedforward size in motion encoder
  dropout: 0.1                        # Dropout rate in motion encoder
  clip_model_name: "ViT-B/32"         # CLIP model variant to use
  activation: "gelu"                  # Activation function in motion encoder

loss:
  temperature: 0.07                   # Temperature parameter for contrastive loss
  margin: 0.2                         # Margin parameter for contrastive loss

checkpoint:
  save_path: "./checkpoints"          # Directory to save model checkpoints
  save_every: 30                      # Save checkpoint every 'save_every' epochs
