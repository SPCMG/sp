{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianan/develop/miniconda3/envs/motionclip/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/HumanML3D/glove/our_vocab_data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m opt \u001b[38;5;241m=\u001b[39m Config()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Initialize dataset and dataloader\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mHumanML3D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Print dataset length\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/sp/data/humanml3d/loader.py:18\u001b[0m, in \u001b[0;36mHumanML3D.__init__\u001b[0;34m(self, opt, split)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_text_len \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mmax_text_len\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_motion_length \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mmax_motion_length\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mWordVectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglove\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mour_vocab\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m     21\u001b[0m split_file \u001b[38;5;241m=\u001b[39m pjoin(opt\u001b[38;5;241m.\u001b[39mdata_root, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/sp/data/humanml3d/utils/word_vectorizer.py:48\u001b[0m, in \u001b[0;36mWordVectorizer.__init__\u001b[0;34m(self, meta_root, prefix)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, meta_root, prefix):\n\u001b[0;32m---> 48\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m_data.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     words \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(pjoin(meta_root, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_words.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39mprefix), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     50\u001b[0m     word2idx \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(pjoin(meta_root, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_idx.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39mprefix), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/develop/miniconda3/envs/motionclip/lib/python3.8/site-packages/numpy/lib/npyio.py:417\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 417\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    418\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/HumanML3D/glove/our_vocab_data.npy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# change the current working directory to the project root\n",
    "os.chdir(\"/home/jianan/projects/sp\")\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from humanml3d.utils.word_vectorizer import WordVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from humanml3d.loader import HumanML3D, collate_fn\n",
    "\n",
    "# Configuration class\n",
    "class Config:\n",
    "    data_root = \"dataset/HumanML3D\"              # Root directory for dataset\n",
    "    motion_dir = \"dataset/HumanML3D/new_joints\"  # Path to motion files\n",
    "    text_dir = \"dataset/HumanML3D/texts\"         # Path to text files\n",
    "    max_text_len = 20\n",
    "    max_motion_length = 100\n",
    "\n",
    "opt = Config()\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = HumanML3D(opt, split=\"train\")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Print dataset length\n",
    "print(f\"Total samples in dataset: {len(dataset)}\")\n",
    "\n",
    "# Motion visualization\n",
    "def visualize_motion(motion):\n",
    "    num_frames, num_features = motion.shape\n",
    "    num_joints = num_features // 3  # Assuming 3D joints\n",
    "    motion = motion.reshape((num_frames, num_joints, 3))\n",
    "\n",
    "    for frame_idx in range(num_frames):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.title(f\"Frame {frame_idx + 1}\")\n",
    "        plt.scatter(motion[frame_idx, :, 0], motion[frame_idx, :, 1], label=\"Joints\")\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.pause(0.1)\n",
    "        plt.clf()\n",
    "\n",
    "# Visualize a few batches\n",
    "for batch_idx, (word_embeddings, pos_one_hots, captions, motions) in enumerate(dataloader):\n",
    "    print(f\"\\nBatch {batch_idx + 1}:\")\n",
    "    print(f\"Word Embeddings Shape: {word_embeddings.shape}\")\n",
    "    print(f\"Motion Shape: {motions.shape}\")\n",
    "    print(f\"Captions: {captions}\")\n",
    "    \n",
    "    # Visualize first motion in batch\n",
    "    visualize_motion(motions[0].numpy())\n",
    "    if batch_idx == 2:  # Test only 3 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motionclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
